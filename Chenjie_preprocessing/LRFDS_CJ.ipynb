{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhxGnAjRTg8P",
        "outputId": "2668e246-e8c9-4215-e68b-c9dfe6290690"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/Progetto/train.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gdown\n",
        "from sklearn.metrics import  confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score,classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder,  StandardScaler, MinMaxScaler,MultiLabelBinarizer,LabelEncoder\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "\n",
        "url = \"https://drive.google.com/drive/folders/1K28d1ufLIZL6WtQIS4CE4H_EZZAzvTqX?usp=drive_link\"\n",
        "gdown.download_folder(url, quiet=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **New Chenjie preprocessing procedure**"
      ],
      "metadata": {
        "id": "GpCVublOal2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train=pd.read_csv('/content/Progetto/train.csv')\n",
        "train=train.drop_duplicates()\n",
        "train.drop(columns=['Cross_Street'],inplace=True)\n",
        "train['Weapon_Used_Code']=train['Weapon_Used_Code'].fillna(train['Weapon_Used_Code'].max()+ 1)\n",
        "mode_victim_sex_value = train['Victim_Sex'].mode().iloc[0]  # Find the most frequent value\n",
        "train['Victim_Sex']=train['Victim_Sex'].fillna(mode_victim_sex_value)\n",
        "mode_victim_descent_value = train['Victim_Descent'].mode().iloc[0]  # Find the most frequent value\n",
        "train['Victim_Descent']=train['Victim_Descent'].fillna(mode_victim_sex_value)\n",
        "mode_modus_operandi_value = train['Modus_Operandi'].mode().iloc[0]  # Find the most frequent value\n",
        "train['Modus_Operandi']=train['Modus_Operandi'].fillna(mode_modus_operandi_value)\n",
        "train['Modus_Operandi']=train['Modus_Operandi'].apply(lambda x:str(x).split(\" \") if x else [])\n",
        "train['Date_Reported'] = pd.to_datetime(train['Date_Reported'],format='%m/%d/%Y %I:%M:%S %p')\n",
        "train['Date_Occurred'] = pd.to_datetime(train['Date_Occurred'],format='%m/%d/%Y %I:%M:%S %p')\n",
        "train['Year_Reported'] = train['Date_Reported'].dt.year\n",
        "train['Month_Reported'] = train['Date_Reported'].dt.month\n",
        "train['Day_Reported'] = train['Date_Reported'].dt.day\n",
        "train['Year_Occurred'] = train['Date_Occurred'].dt.year\n",
        "train['Month_Occurred'] = train['Date_Occurred'].dt.month\n",
        "train['Day_Occurred'] = train['Date_Occurred'].dt.day\n",
        "train['Time_Occurred'] = train['Time_Occurred'].apply(lambda x: int(x // 100))\n",
        "label_encoder = LabelEncoder()\n",
        "train['Crime_Category'] = label_encoder.fit_transform(train['Crime_Category'])\n",
        "train.drop(columns=['Year_Occurred'],inplace=True)\n",
        "corr_df = train.corr(numeric_only=True)\n",
        "columns_to_convert = ['Latitude', 'Longitude', 'Area_ID','Reporting_District_no', 'Part 1-2', 'Victim_Age','Premise_Code','Weapon_Used_Code']\n",
        "train[columns_to_convert] = train[columns_to_convert].astype(int)\n",
        "included_columns=['Location', 'Latitude', 'Longitude','Area_ID','Reporting_District_no','Part 1-2','Modus_Operandi', 'Victim_Age','Victim_Sex', 'Victim_Descent', 'Premise_Code','Weapon_Used_Code','Status','Crime_Category', 'Year_Reported','Month_Reported', 'Day_Reported', 'Month_Occurred', 'Day_Occurred']\n",
        "new_train=train[included_columns]\n",
        "X = new_train.drop('Crime_Category', axis=1)\n",
        "y = new_train['Crime_Category']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "CCwOu4DMTxGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Transformer for MultiLabelBinarizer for the feature Modus Operandi, which is multilabeled\n",
        "class MultiLabelBinarizerTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.ml_binarizers = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fit a MultiLabelBinarizer for each column in the input DataFrame.\n",
        "        \"\"\"\n",
        "        for column in X.columns:\n",
        "            mlb = MultiLabelBinarizer()\n",
        "            mlb.fit(X[column])\n",
        "            self.ml_binarizers[column] = mlb\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Transform the input DataFrame by applying the corresponding MultiLabelBinarizer\n",
        "        for each column and concatenating the binary matrices.\n",
        "        \"\"\"\n",
        "        X_transformed = []\n",
        "        for column in X.columns:\n",
        "            mlb = self.ml_binarizers[column]\n",
        "            transformed_data = mlb.transform(X[column])\n",
        "            # Create a DataFrame with meaningful column names\n",
        "            transformed_df = pd.DataFrame(\n",
        "                transformed_data,\n",
        "                columns=[f\"{column}_{cls}\" for cls in mlb.classes_]\n",
        "            )\n",
        "            X_transformed.append(transformed_df)\n",
        "        # Concatenate all transformed columns\n",
        "        return pd.concat(X_transformed, axis=1)\n",
        "# Define ColumnTransformer with merged transformers\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore'),['Area_ID','Reporting_District_no','Part 1-2','Victim_Sex','Victim_Descent','Premise_Code','Weapon_Used_Code','Status','Victim_Sex','Location']),\n",
        "        ('minmax_scaler', MinMaxScaler(),['Latitude', 'Longitude','Victim_Age'])\n",
        "        , ('modus_operandi', MultiLabelBinarizerTransformer(),['Modus_Operandi'])\n",
        "        ],\n",
        "        remainder='passthrough'  # Pass through any remaining columns that are not specified in transformers\n",
        "        )\n",
        "pipeline = make_pipeline(\n",
        "    preprocessor,\n",
        "    StandardScaler(with_mean=False),\n",
        "    LogisticRegression()\n",
        ")"
      ],
      "metadata": {
        "id": "OElHdGxgTzRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid based on the logisticregression model\n",
        "grid = {\n",
        "    'logisticregression__C': [0.001, 0.01, 0.1, 1, 10],\n",
        "    'logisticregression__penalty': ['l1', 'l2'],\n",
        "    'logisticregression__solver': ['liblinear', 'newton-cg']\n",
        "}\n",
        "\n",
        "pipe_cv = GridSearchCV(pipeline,grid,cv=2,verbose=1,n_jobs=-1)\n",
        "if pipe_cv is not None:\n",
        "    pipe_cv.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Best score: {pipe_cv.best_score_}\")\n",
        "    for hp, val in pipe_cv.best_params_.items():\n",
        "        print(f\"{hp}: {val}\")\n",
        "\n",
        "y_pred = pipe_cv.predict(X_test)\n",
        "print(\"Logistic Regression Model:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MKr_996T0nZ",
        "outputId": "5ef02a53-e5d8-46f3-cf27-f58bdab6afb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 2 folds for each of 20 candidates, totalling 40 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy 93.22%"
      ],
      "metadata": {
        "id": "jcVlv8wvT3t4"
      }
    }
  ]
}